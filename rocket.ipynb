{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "from numba import njit\n",
    "import xgboost as xgb\n",
    "from fastai_timeseries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files\n",
    "\n",
    "files = glob.glob('data.*')\n",
    "files_train = files[:-2]\n",
    "files_test = files[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast standardization\n",
    "\n",
    "@njit(parallel = True)\n",
    "def standardize(X_train,X_test,means,stds):\n",
    "    for i in range(X_train.shape[1]):\n",
    "        X_train[:,i] = (X_train[:,i] - means[i]) / stds[i]\n",
    "        X_test[:,i] = (X_test[:,i] - means[i]) / stds[i]\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / Testing batch\n",
    "\n",
    "columns = ['time', 'b1', 'b2', 'b3', 'b4', 'b5', 'b6', 'b7', 'b8', 'b9',\n",
    "       'b10', 'b11', 'b12', 'b13', 'b14', 'b15', 'b16', 'b17', 'b18', 'b19',\n",
    "       'b20', 'b21', 'b22', 'b23', 'b24', 'b25', 'b26', 'b27', 'b28', 'b29',\n",
    "       'b30', 'b31', 'b32', 'b33', 'b34', 'b35', 'b36', 'b37', 'b38', 'b39',\n",
    "       'b40', 'b41', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9',\n",
    "       's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19',\n",
    "       's20', 's21', 's22', 's23', 's24', 's25', 's26', 's27', 's28', 's29',\n",
    "       's30', 's31', 's32', 's33', 's34', 's35', 's36', 's37', 's38', 's39',\n",
    "       's40', 's41', 's42', 's43', 's44', 's45', 's46', 's47', 's48', 's49',\n",
    "       's50', 'y_1m', 'y_5m', 'y_10m', 'y_15m', 'y_30m', 'y_hat1', 'y_hat2',\n",
    "       'y_hat3']\n",
    "for i in range(len(files_train)):\n",
    "    print(files_train[i])\n",
    "    #i=0\n",
    "    #File loading\n",
    "    if i == 0:\n",
    "        df = pd.read_csv(files_train[i])\n",
    "        df.columns = columns\n",
    "    else:\n",
    "        df = pd.read_csv(files_train[i],names=columns)\n",
    "    test = pd.read_csv(files_test[i],header=None,names=columns)\n",
    "\n",
    "    #Preprocessing\n",
    "    df = df.set_index(pd.DatetimeIndex(pd.to_datetime(df[df.columns[0]])))\n",
    "    target = df.y_10m\n",
    "    df = df.drop(['s19','s24','time',\"y_1m\",\"y_5m\",\"y_10m\",\"y_15m\",\"y_30m\",\"y_hat1\",\"y_hat2\",\"y_hat3\"],axis=1)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    test = test.set_index(pd.DatetimeIndex(pd.to_datetime(test[test.columns[0]])))\n",
    "    target_test = test.y_10m\n",
    "    test = test.drop(['s19','s24','time',\"y_1m\",\"y_5m\",\"y_10m\",\"y_15m\",\"y_30m\",\"y_hat1\",\"y_hat2\",\"y_hat3\"],axis=1)\n",
    "    test = test.fillna(0)\n",
    "    test2 = np.array(test)\n",
    "\n",
    "    y_train = np.array(target).reshape(len(target),)\n",
    "    kernels = generate_kernels(df.shape[1], 1000)\n",
    "\n",
    "    means = np.mean(np.array(df.tail(100000)),axis=1)\n",
    "    stds = np.std(np.array(df.tail(100000)),axis=1)\n",
    "    X_train, X_test = standardize(np.array(df.tail(100000)),test2,means,stds)\n",
    "\n",
    "    # (2) then transform the normalised time series\n",
    "    X_training_transform = apply_kernels(X_train, kernels)\n",
    "    gc.collect()\n",
    "    \n",
    "    classifier = xgb.XGBRegressor(max_depth=3,\n",
    "                              learning_rate=0.1,\n",
    "                               n_estimators=100,\n",
    "                               verbosity=2,\n",
    "                               booster='gbtree',\n",
    "                               tree_method='auto',\n",
    "                               n_jobs=7,\n",
    "                               gpu_id=0,\n",
    "                               gamma=0,\n",
    "                               min_child_weight=1,\n",
    "                               max_delta_step=4,\n",
    "                               subsample=.5,\n",
    "                               colsample_bytree=1,\n",
    "                               colsample_bylevel=1,\n",
    "                               colsample_bynode=1,\n",
    "                               reg_alpha=0,\n",
    "                               reg_lambda=1,\n",
    "                               scale_pos_weight=1,\n",
    "                               base_score=0.5,\n",
    "                               random_state=0,\n",
    "                               missing=None)\n",
    "                               #weight=np.sqrt(target_train**2)))\n",
    "    for j in range(0,int(X_test.shape[0]/50000)+1):\n",
    "        index = min(X_test.shape[0],(j+1)*50000)\n",
    "        X_test_transform = apply_kernels(X_test[(j*50000):index], kernels)\n",
    "        means = np.mean(X_train,axis=1)\n",
    "        stds = np.std(X_train,axis=1)\n",
    "        X_training_transform, X_test_transform = standardize(X_training_transform,X_test_transform,means,stds)\n",
    "        if j == 0:\n",
    "            print('training...')\n",
    "            classifier.fit(X_training_transform, y_train[-100000:])\n",
    "        print('testing:' + str(j) + '...')\n",
    "        preds = classifier.predict(X_test_transform)\n",
    "        if j == 0:\n",
    "            all_preds = preds\n",
    "        else:\n",
    "            all_preds = np.concatenate((all_preds,preds))\n",
    "        gc.collect()\n",
    "    lin = np.linspace(0,1,5)\n",
    "    print(pd.DataFrame(preds).quantile(lin))\n",
    "    print(\"cor: \" + str(np.corrcoef(target_test,all_preds)[0,1]))\n",
    "    pd.DataFrame({'index':test.index,'y_pred':all_preds,'target_test':target_test}).to_csv(\"rocket\" + str(i) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list files\n",
    "\n",
    "files = glob.glob('rocket*')[2:11]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate predictions\n",
    "\n",
    "for i,file in enumerate(files):\n",
    "    df = pd.read_csv(file)\n",
    "    df = df.set_index(pd.DatetimeIndex(pd.to_datetime(df[df.columns[0]]))) \n",
    "    df =df.y_pred\n",
    "    if i == 0:\n",
    "        all_df = df\n",
    "    else:\n",
    "        all_df = pd.concat((all_df,df))\n",
    "all_df.to_csv('all_rocket.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
